# â˜• CafÃ©IA - GraphRAG avec Ollama

Interface graphique Streamlit pour interroger un LLM local via Ollama et alimenter un systÃ¨me RAG (Retrieval-Augmented Generation) avec diffÃ©rents types de documents.

## ğŸš€ FonctionnalitÃ©s

- ğŸ“¤ **Upload de documents** multiples formats : PDF, DOCX, XLSX, TXT
- ğŸ¤– **Interrogation LLM** via Ollama (modÃ¨le qwen2.5:14b)
- ğŸ” **4 modes de recherche** :
  - **Naive** : RAG classique, recherche simple
  - **Local** : EntitÃ©s et relations proches (1 hop)
  - **Global** : Patterns globaux du knowledge graph
  - **Hybrid** : Combinaison local + global (recommandÃ© pour multi-hop)
- ğŸ“œ **Historique** des requÃªtes avec horodatage
- ğŸ¨ **Interface moderne** et intuitive

## ğŸ“‹ PrÃ©requis

### 1. Ollama installÃ© et lancÃ©

```bash
# Installer Ollama (si pas dÃ©jÃ  fait)
curl -fsSL https://ollama.com/install.sh | sh

# Lancer Ollama
ollama serve
```

### 2. TÃ©lÃ©charger les modÃ¨les nÃ©cessaires

```bash
# ModÃ¨le LLM (32K context)
ollama pull qwen2.5:14b

# ModÃ¨le d'embeddings
ollama pull nomic-embed-text
```

### 3. Python 3.11+

VÃ©rifiez votre version de Python :
```bash
python3 --version
```

## ğŸ› ï¸ Installation

### 1. Activer l'environnement virtuel

```bash
source venv/bin/activate
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

## ğŸ¯ Utilisation

### Lancer l'application Streamlit

```bash
streamlit run app.py
```

L'application s'ouvrira automatiquement dans votre navigateur par dÃ©faut Ã  l'adresse : `http://localhost:8501`

### Workflow recommandÃ©

1. **Uploader des documents** (onglet "ğŸ“¤ Upload Documents")
   - Glissez-dÃ©posez vos fichiers PDF, DOCX, XLSX ou TXT
   - Cliquez sur "ğŸš€ Indexer les documents"
   - Attendez la confirmation d'indexation

2. **Interroger le RAG** (onglet "ğŸ’¬ Interroger le RAG")
   - Saisissez votre question
   - Choisissez le mode de recherche (hybrid recommandÃ©)
   - Cliquez sur "ğŸ” Rechercher"

3. **Consulter l'historique** (onglet "ğŸ“œ Historique")
   - Visualisez toutes vos requÃªtes prÃ©cÃ©dentes
   - Consultez les rÃ©ponses obtenues

## ğŸ“ Structure du projet

```
CafÃ©IA/
â”œâ”€â”€ app.py                  # Interface Streamlit principale
â”œâ”€â”€ main.py                 # Configuration LightRAG + Ollama
â”œâ”€â”€ document_processor.py   # Extraction de texte (PDF, DOCX, XLSX, TXT)
â”œâ”€â”€ insert_docs.py          # Script d'insertion de documents (CLI)
â”œâ”€â”€ query_demo.py           # Script de dÃ©monstration des requÃªtes (CLI)
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ storage/                # Dossier du knowledge graph (crÃ©Ã© automatiquement)
â””â”€â”€ venv/                   # Environnement virtuel Python
```

## ğŸ”§ Configuration avancÃ©e

Vous pouvez modifier les paramÃ¨tres dans [main.py](main.py) :

- **ModÃ¨le LLM** : `llm_model_name="qwen2.5:14b"`
- **Context window** : `llm_model_max_token_size=32768`
- **ParallÃ©lisme** : `llm_model_max_async=2`
- **Dimensions embeddings** : `embedding_dim=768`

## ğŸ› DÃ©pannage

### Erreur "Connection refused" lors du lancement

- VÃ©rifiez qu'Ollama est bien lancÃ© : `ollama serve`
- VÃ©rifiez que les modÃ¨les sont tÃ©lÃ©chargÃ©s : `ollama list`

### Erreur lors de l'extraction de documents

- Assurez-vous que toutes les dÃ©pendances sont installÃ©es :
  ```bash
  pip install PyPDF2 python-docx openpyxl
  ```

### RÃ©initialiser le knowledge graph

Si vous voulez repartir de zÃ©ro :
```bash
rm -rf storage/
```

## ğŸš€ Scripts CLI (optionnels)

### InsÃ©rer des documents en ligne de commande

```bash
python insert_docs.py
```

### Tester les diffÃ©rents modes de requÃªte

```bash
python query_demo.py
```

## ğŸ“š Documentation

- [LightRAG Documentation](https://github.com/HKUDS/LightRAG)
- [Ollama Documentation](https://ollama.com/docs)
- [Streamlit Documentation](https://docs.streamlit.io)

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## ğŸ“ Licence

Ce projet est fourni Ã  titre Ã©ducatif et de dÃ©monstration.

---

**â˜• DÃ©veloppÃ© avec passion pour CafÃ©IA**
